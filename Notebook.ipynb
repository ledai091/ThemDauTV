{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import csv\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim.lr_scheduler as LRSchedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3624432\n"
     ]
    }
   ],
   "source": [
    "with open(\"train_tieng_viet.txt\", 'r', encoding='utf-8') as f:\n",
    "    train_outputs = f.readlines()\n",
    "print(len(train_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Di mot ngay dang hoc 1 sang khong\n"
     ]
    }
   ],
   "source": [
    "def remove_tone_line(utf8_str):\n",
    "    intab_l = \"ạảãàáâậầấẩẫăắằặẳẵóòọõỏôộổỗồốơờớợởỡéèẻẹẽêếềệểễúùụủũưựữửừứíìịỉĩýỳỷỵỹđ\"\n",
    "    intab_u = \"ẠẢÃÀÁÂẬẦẤẨẪĂẮẰẶẲẴÓÒỌÕỎÔỘỔỖỒỐƠỜỚỢỞỠÉÈẺẸẼÊẾỀỆỂỄÚÙỤỦŨƯỰỮỬỪỨÍÌỊỈĨÝỲỶỴỸĐ\"\n",
    "    intab = list(intab_l + intab_u)\n",
    "    outtab_l = \"a\"*17 + \"o\"*17 + \"e\"*11 + \"u\"*11 + \"i\"*5 + \"y\"*5 + \"d\"\n",
    "    outtab_u = \"A\"*17 + \"O\"*17 + \"E\"*11 + \"U\"*11 + \"I\"*5 + \"Y\"*5 + \"D\"\n",
    "    outtab = outtab_l + outtab_u\n",
    "    r = re.compile(\"|\".join(intab))\n",
    "    replaces_dict = dict(zip(intab, outtab))\n",
    "    non_dia_str = r.sub(lambda m: replaces_dict[m.group(0)], utf8_str)\n",
    "    return non_dia_str\n",
    "\n",
    "print(remove_tone_line(\"Đi một ngày đàng học 1 sàng không\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx_500k, train_ipt_500k, train_opt_500k = [], [], []\n",
    "val_idx_50k, val_ipt_50k, val_opt_50k = [], [], []\n",
    "test_idx_50k, test_ipt_50k, test_opt_50k = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/600000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600000/600000 [00:24<00:00, 24837.84it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(600000)):\n",
    "    [idx, origin_seq] = train_outputs[i].split('\\t')\n",
    "    try:\n",
    "        non_acc_seq = remove_tone_line(origin_seq)\n",
    "    except:\n",
    "        print(f\"Error at {i}\")\n",
    "        next\n",
    "    if i < 500000:\n",
    "        train_idx_500k.append(idx)\n",
    "        train_opt_500k.append(origin_seq)\n",
    "        train_ipt_500k.append(non_acc_seq)\n",
    "    elif i < 550000:\n",
    "        val_idx_50k.append(idx)\n",
    "        val_opt_50k.append(origin_seq)\n",
    "        val_ipt_50k.append(non_acc_seq)\n",
    "    else:\n",
    "        test_idx_50k.append(idx)\n",
    "        test_opt_50k.append(origin_seq)\n",
    "        test_ipt_50k.append(non_acc_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Build Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input:List[List[str]], output: List[List[str]], vocab_input, vocab_output) -> None:\n",
    "        self.vocab_input = vocab_input\n",
    "        self.vocab_output = vocab_output\n",
    "        self.input = [[vocab_input.encode(token) for token in text.split()] for text in input]\n",
    "        self.output = [[vocab_output.encode(token) for token in text.split()] for text in output]\n",
    "        self._add_start_end()\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    def _add_start_end(self):\n",
    "        self.input = [[len(self.vocab_input)] + text + [len(self.vocab_input) + 1] for text in self.input]\n",
    "        self.output = [[len(self.vocab_output)] + text + [len(self.vocab_output) + 1] for text in self.output]\n",
    "    def __getitem__(self, idx):\n",
    "        input = self.input[idx]\n",
    "        output = self.input[idx]\n",
    "        input = torch.tensor(input, dtype=torch.float32)\n",
    "        output = torch.tensor(output, dtype=torch.float32)\n",
    "        return input, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildVocab:\n",
    "    def __init__(self, data):\n",
    "        self.vocab = build_vocab_from_iterator(self.input_iterator(data), specials=[\"<unk>\"])\n",
    "        self.vocab.set_default_index(self.vocab[\"<unk>\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def input_iterator(self, data):\n",
    "        for text in data:\n",
    "            yield text.split()\n",
    "    def encode(self, token):\n",
    "        return self.vocab[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_input = BuildVocab(train_ipt_500k)\n",
    "vocab_output = BuildVocab(train_opt_500k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_ipt_500k, train_opt_500k, vocab_input, vocab_output)\n",
    "val_dataset = CustomDataset(val_ipt_50k, val_opt_50k, vocab_input, vocab_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    input, output = zip(*batch)\n",
    "    max_len_input = max([len(sample) for sample in input])\n",
    "    max_len_output = max([len(sample) for sample in output])\n",
    "    input_padded = []\n",
    "    output_padded = []\n",
    "    for sample in input:\n",
    "        if len(sample) < max_len_input:\n",
    "            num_paddings = max_len_input - len(sample)\n",
    "            padded =torch.cat([sample,torch.tensor([0]*num_paddings)])\n",
    "        else:\n",
    "            padded = sample[:max_len_input]\n",
    "        input_padded.append(padded)\n",
    "        \n",
    "    for sample in output:\n",
    "        if len(sample) < max_len_output:\n",
    "            num_paddings = max_len_output - len(sample)\n",
    "            padded =torch.cat([sample,torch.tensor([0]*num_paddings)])\n",
    "        else:\n",
    "            padded = sample[:max_len_output]\n",
    "        output_padded.append(padded)\n",
    "    input_padded = torch.stack(input_padded)\n",
    "    output_padded = torch.stack(output_padded)\n",
    "    return input_padded, output_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Position Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$PE(pos, 2i)$ = $sin(\\frac{pos}{1000^{\\frac{2i}{d_{model}}}})$\\\n",
    "\\\n",
    "$PE(pos, 2i+1)$ = $cos(\\frac{pos}{1000^{\\frac{2i}{d_{model}}}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1/np.power(1000, (2*(i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:,np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis,:],\n",
    "                            d_model)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return torch.tensor(pos_encoding, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Masking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    mask = (seq == 0).float()\n",
    "    return mask.unsqueeze(1).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(seq_len):\n",
    "    ahead_mask = 1 - torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "    ahead_mask = ahead_mask == 0\n",
    "    return ahead_mask.unsqueeze(0).unsqueeze(0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scaled Dot Product Attention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Attention(Q, K, V)$ = $softmax_k(\\frac{QK^T}{\\sqrt{d_k}}V)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = torch.matmul(q, k.transpose(-2, -1))  # Correct matrix multiplication\n",
    "    \n",
    "    dk = k.shape[-1]\n",
    "    scaled_attention_logits = matmul_qk / torch.sqrt(torch.tensor(dk))\n",
    "    \n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    \n",
    "    attention_weight = F.softmax(scaled_attention_logits)\n",
    "    output = torch.matmul(attention_weight, v)\n",
    "    return output, attention_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled_attention_logit:  tensor([[ 1.0000e+00, -1.0000e+09, -1.0000e+09],\n",
      "        [ 1.0000e+00,  2.0000e+00, -1.0000e+09],\n",
      "        [ 1.0000e+00,  1.0000e+00,  5.0000e+00]])\n",
      "attention_weights:  tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.2689, 0.7311, 0.0000],\n",
      "        [0.0177, 0.0177, 0.9647]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1869296/3265515529.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_weights = F.softmax(scaled_attention_logit)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.tensor([[0, 1, 1],\n",
    "                    [0, 0, 1],\n",
    "                    [0, 0, 0]], dtype = torch.float32)\n",
    "\n",
    "scaled_attention_logit = torch.tensor([[1, 3, 10],\n",
    "                                        [1, 2, 5],\n",
    "                                        [1, 1, 5]], dtype = torch.float32)\n",
    "\n",
    "scaled_attention_logit += (mask * -1e9)\n",
    "attention_weights = F.softmax(scaled_attention_logit)\n",
    "print('scaled_attention_logit: ', scaled_attention_logit)\n",
    "print('attention_weights: ', attention_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tensor([[    0.00000,     1.00000,     0.00000,     0.00000]])\n",
      "Output is:\n",
      "tensor([[   10.00000,     0.00000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1869296/912260247.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_weight = F.softmax(scaled_attention_logits)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(precision=5, sci_mode=False)\n",
    "def print_out(q, k, v):\n",
    "  temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "  print ('Attention weights are:')\n",
    "  print (temp_attn)\n",
    "  print ('Output is:')\n",
    "  print (temp_out)\n",
    "temp_k = torch.tensor([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=torch.float32)  # (4, 3)\n",
    "\n",
    "temp_v = torch.tensor([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=torch.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = torch.tensor([[0, 10, 0]], dtype=torch.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multihead Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        self.wk = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        self.wv = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        \n",
    "        self.linear = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.reshape(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.transpose(1, 2)\n",
    "    def forward(self, v, k, q, mask):\n",
    "        batch_size = q.shape[0]\n",
    "        \n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        scaled_attention = scaled_attention.permute(0, 2, 1, 3) # (batch_size, seq_len num_heads, depth)\n",
    "        concat_attention = scaled_attention.reshape(batch_size, -1, self.d_model)\n",
    "        output = self.linear(concat_attention)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1869296/912260247.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_weight = F.softmax(scaled_attention_logits)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 60, 512]), torch.Size([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = torch.rand((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feed Forward Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffn(d_model, dff):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(d_model, dff),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(dff, d_model)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoder layer**\n",
    "1. Multi-head Attention (với padding mask)\n",
    "2. Point wise feed forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = ffn(d_model, dff)\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output, _  = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layer_norm1(x + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layer_norm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1869296/912260247.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_weight = F.softmax(scaled_attention_logits)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "   torch.rand((64, 43, 512)), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 43, 512])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoder**\n",
    "1. Input Embedding\n",
    "2. Positional Encoding\n",
    "3. N encoder layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maxinum_pe, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.pe = position_encoding(maxinum_pe, self.d_model)\n",
    "        \n",
    "        self.enc_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, dff, dropout) for _ in range(num_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        x = self.embedding(x.type(torch.int))\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model))\n",
    "        x += self.pe[:, :seq_len:, :].to(x.device)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 62, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1869296/912260247.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_weight = F.softmax(scaled_attention_logits)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, input_vocab_size=8500,\n",
    "                         maxinum_pe=10000)\n",
    "\n",
    "# Init sample tensorflow with shape 64 x 62 and data type int.\n",
    "temp_input = torch.randint(0, 2000, (64, 62))\n",
    "\n",
    "sample_encoder_output = sample_encoder(temp_input, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decoder Layer**\n",
    "1. Masked multi-head attention (với look ahead mask và padding mask).\n",
    "2. Multi-head attention (với padding mask). Ma trận V, K cùng lấy output từ Encoder và ma trận  Q nhận output từ masked multi-head attention.\n",
    "3. Point wise feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout = 0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = ffn(d_model, dff)\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.layer_norm3 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoding_output, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weight_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1)\n",
    "        out1 = self.layer_norm1(attn1)\n",
    "        \n",
    "        attn2, attn_weight_block2 = self.mha2(encoding_output, encoding_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2)\n",
    "        out2 = self.layer_norm2(attn2)\n",
    "        \n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.layer_norm3(out2 + ffn_output)\n",
    "        \n",
    "        return out3, attn_weight_block1, attn_weight_block2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decoder**\n",
    "1. Output Embedding\n",
    "2. Positional Embedding\n",
    "3. N encoder layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_pe, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pe = position_encoding(maximum_pe, d_model)\n",
    "        \n",
    "        self.dec_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, dff, dropout) for _ in range(self.num_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, look_ahead_mask, padding_mask):\n",
    "        seq_len = x.shape[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "        x = self.embedding(x.type(torch.int))\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model))\n",
    "        x += self.pe[:, :seq_len, :].to(x.device)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x , block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n",
    "            attention_weights[f'decoder_layer{i}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i}_block2'] = block2\n",
    "            \n",
    "        return x, attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1869296/912260247.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_weight = F.softmax(scaled_attention_logits)\n"
     ]
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000,\n",
    "                         maximum_pe=5000)\n",
    "temp_input = torch.randint(0, 200, (64, 26))\n",
    "\n",
    "output, attn = sample_decoder(temp_input, \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              look_ahead_mask=None, \n",
    "                              padding_mask=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, dropout)\n",
    "        \n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, dropout)\n",
    "        \n",
    "        self.final_layer = nn.Linear(d_model, target_vocab_size)\n",
    "        \n",
    "    def forward(self, input, target, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(input, enc_padding_mask)\n",
    "        \n",
    "        dec_output, attention_weights = self.decoder(target, enc_output, look_ahead_mask, dec_padding_mask)\n",
    "        \n",
    "        final_output = self.final_layer(dec_output)\n",
    "        \n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1869296/912260247.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_weight = F.softmax(scaled_attention_logits)\n"
     ]
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "    input_vocab_size=8500, target_vocab_size=8000, \n",
    "    pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = torch.randint(0, 200, (64, 38))\n",
    "temp_target = torch.randint(0, 200, (64, 36))\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = len(vocab_input) + 2\n",
    "target_vocab_size = len(vocab_output)+ 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff,\n",
    "                          input_vocab_size=input_vocab_size, target_vocab_size=target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          dropout=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daile/anaconda3/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "class CustomSchedule(torch.optim.lr_scheduler.LRScheduler):\n",
    "    def __init__(self,optimizer, d_model, warmup_steps=4000, last_epoch=-1):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "        \n",
    "    def get_lr(self):\n",
    "        step = max(1, self.last_epoch + 1)\n",
    "        arg1 = step ** (-0.5)\n",
    "        arg2 = step * (self.warmup_steps ** (-1.5))\n",
    "        scale = torch.sqrt(torch.tensor(self.d_model)) * min(arg1, arg2)\n",
    "        return [base_lr * scale for base_lr  in self.base_lrs]\n",
    "optimizer =  Adam(transformer.parameters())\n",
    "schedule = CustomSchedule(optimizer, d_model=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loss Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_hat, y):\n",
    "    mask = (y != 0)\n",
    "    loss = F.cross_entropy(y_hat.transpose(1, 2), y)\n",
    "    loss *= mask.float()\n",
    "    return loss.sum() / mask.float().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(input, target, device):\n",
    "    enc_padding_mask = create_padding_mask(input)\n",
    "    look_ahead_mask = create_look_ahead_mask(target.size(1)).type(torch.float)\n",
    "    dec_padding_mask = create_padding_mask(target)\n",
    "    combined_mask = torch.maximum(dec_padding_mask.to(device), look_ahead_mask.to(device))\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader, scheduler, epochs, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        for _, (input, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "            enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(input, target, device)\n",
    "            output, _ = model(input, target, enc_padding_mask, look_ahead_mask, dec_padding_mask)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.item()\n",
    "        train_losses.append(train_loss/len(train_loader))\n",
    "        for _, (input, target) in enumerate(val_loader):\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "            enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(input, target)\n",
    "            output, _ = model(input, target, enc_padding_mask, look_ahead_mask, dec_padding_mask)\n",
    "            loss = loss_fn(output, target)\n",
    "            val_loss += loss.item()\n",
    "        val_losses.append(val_loss/len(val_loader))\n",
    "        print(f'Epoch {epoch + 1}: Training Loss: {train_losses[-1]} - Validation Loss: {val_losses[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "train(transformer, optimizer, loss_function, train_loader, val_loader, schedule, 10, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
